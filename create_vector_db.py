import os
import json
import time
from tqdm import tqdm
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma

# ==========================================================
# [ì„¤ì •] ê²½ë¡œ
# ==========================================================
INPUT_JSON_PATH = r"C:\Users\owner\myvenv\legal_data_total_vlm.json"
# ì €ì¥í•  í´ë” ì´ë¦„ 2ê°œ
DB_PATH_1 = r"C:\Users\owner\myvenv\chroma_db_part1"
DB_PATH_2 = r"C:\Users\owner\myvenv\chroma_db_part2"

os.environ["GOOGLE_API_KEY"] = "AIzaSyCYDsHspn7XQm5pcGi6iKZVThqiNp_Xm4M"

def create_split_vector_db():
    print("ğŸš€ [ë¶„í•  ëª¨ë“œ] ë²¡í„° DBë¥¼ 2ê°œë¡œ ìª¼ê°œì„œ ë§Œë“­ë‹ˆë‹¤...")

    # 1. JSON ë¡œë“œ
    if not os.path.exists(INPUT_JSON_PATH):
        print("âŒ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    total_len = len(data)
    print(f"ğŸ“Š ì´ ë°ì´í„°: {total_len}ê°œ")
    
    # 2. ë°ì´í„°ë¥¼ ì •í™•íˆ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
    mid_index = total_len // 2
    data_part1 = data[:mid_index]
    data_part2 = data[mid_index:]
    
    print(f"   - Part 1: {len(data_part1)}ê°œ -> {DB_PATH_1}")
    print(f"   - Part 2: {len(data_part2)}ê°œ -> {DB_PATH_2}")

    # 3. ì„ë² ë”© ëª¨ë¸
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

    # 4. í•¨ìˆ˜ ì •ì˜ (DB ìƒì„±ìš©)
    def process_and_save(data_chunk, save_path, start_index_offset):
        texts = []
        ids = []
        metadatas = []
        
        # ë°ì´í„° ê°€ê³µ
        for idx, item in enumerate(data_chunk):
            real_idx = start_index_offset + idx # ì „ì²´ ê¸°ì¤€ ì¸ë±ìŠ¤ (Lookupìš©)
            
            content = item.get('content', '').strip()
            source = item.get('source', '').strip()
            article = item.get('article', '').strip()
            
            if not content: continue
            
            full_text = f"[{source}] [{article}] {content}"
            texts.append(full_text)
            ids.append(str(real_idx)) # Lookupì„ ìœ„í•´ ì›ë³¸ ì¸ë±ìŠ¤ ì €ì¥
            metadatas.append({"source": source, "article": article})

        # ë°°ì¹˜ ì²˜ë¦¬ ë° ì €ì¥
        batch_size = 100
        first_batch = True
        vector_store = None
        
        print(f"ğŸ‘‰ '{save_path}' ìƒì„± ì¤‘...")
        for i in tqdm(range(0, len(texts), batch_size), desc="   ì €ì¥ ì¤‘"):
            b_texts = texts[i : i+batch_size]
            b_ids = ids[i : i+batch_size]
            b_metas = metadatas[i : i+batch_size]
            
            if not b_texts: continue
            
            b_embeddings = embeddings.embed_documents(b_texts)
            
            if first_batch:
                vector_store = Chroma(
                    embedding_function=embeddings,
                    persist_directory=save_path,
                    collection_name="construction_laws"
                )
                first_batch = False
            
            vector_store._collection.add(
                ids=b_ids,
                embeddings=b_embeddings,
                metadatas=b_metas,
                documents=b_ids
            )
            time.sleep(0.5)

    # 5. ì‹¤í–‰
    process_and_save(data_part1, DB_PATH_1, 0)          # 0ë²ˆë¶€í„° ì‹œì‘
    process_and_save(data_part2, DB_PATH_2, mid_index)  # ì¤‘ê°„ë²ˆí˜¸ë¶€í„° ì‹œì‘

    print(f"\nğŸ‰ ì„±ê³µ! ë‘ ê°œì˜ í´ë”ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"1. {DB_PATH_1}")
    print(f"2. {DB_PATH_2}")
    print("ì´ì œ ê°ê° 70MB ì •ë„ì¼ ê²ë‹ˆë‹¤. GitHubì— ë‘˜ ë‹¤ ì˜¬ë¦¬ì„¸ìš”!")

if __name__ == "__main__":
    create_split_vector_db()